{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba3bd59-70b3-4ea3-9648-7102048f4472",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/26 23:42:52 WARN Utils: Your hostname, MININT-7B4IC2U resolves to a loopback address: 127.0.1.1; using 192.168.56.1 instead (on interface eth2)\n",
      "23/11/26 23:42:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/26 23:42:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "cluster = SparkContext(\"local\", \"BasicWordCount\")\n",
    "\n",
    "\n",
    "readf = cluster.textFile(\"/mnt/c/Users/Tarun/Desktop/DIC/HW-3 BONUS/pg71885.txt\")\n",
    "\n",
    "\n",
    "word_counts = readf.flatMap(lambda line: line.split()) \\\n",
    "                     .map(lambda word: (word.lower(), 1)) \\\n",
    "                     .reduceByKey(lambda x, y: x + y) \\\n",
    "                    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "word_counts.saveAsTextFile(\"basic_wcount.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2c11558-e72a-4f8f-b623-3e17c9001866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word - the           -        Count - 13926\n",
      "Word - of           -        Count - 11847\n",
      "Word - to           -        Count - 5107\n",
      "Word - and           -        Count - 4484\n",
      "Word - in           -        Count - 4268\n",
      "Word - is           -        Count - 3900\n",
      "Word - a           -        Count - 3667\n",
      "Word - as           -        Count - 2883\n",
      "Word - that           -        Count - 2541\n",
      "Word - it           -        Count - 2309\n",
      "Word - be           -        Count - 2237\n",
      "Word - we           -        Count - 2230\n",
      "Word - which           -        Count - 2092\n",
      "Word - for           -        Count - 1480\n",
      "Word - by           -        Count - 1451\n",
      "Word - our           -        Count - 1328\n",
      "Word - are           -        Count - 1327\n",
      "Word - with           -        Count - 1324\n",
      "Word - or           -        Count - 1241\n",
      "Word - not           -        Count - 1197\n",
      "Word - its           -        Count - 1045\n",
      "Word - this           -        Count - 1036\n",
      "Word - an           -        Count - 933\n",
      "Word - from           -        Count - 902\n",
      "Word - have           -        Count - 888\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "with open('./basic_wcount.txt/part-00000', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "top_25 = lines[:25]\n",
    "\n",
    "for line in top_25:\n",
    "    word, count = line.strip(\"()\\n'\").split(', ')\n",
    "    print(\"Word - {}           -        Count - {}\".format(word.strip('\\''), count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee0000d-390f-4424-b6e5-f0bad1f1dc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/27 00:46:30 WARN Utils: Your hostname, MININT-7B4IC2U resolves to a loopback address: 127.0.1.1; using 192.168.56.1 instead (on interface eth2)\n",
      "23/11/27 00:46:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/27 00:46:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import string\n",
    "\n",
    "sc = SparkContext(\"local\", \"CountWord\")\n",
    "\n",
    "\n",
    "readf1 = sc.textFile(\"/mnt/c/Users/Tarun/Desktop/DIC/HW-3 BONUS/pg71913.txt\")\n",
    "readf2 = sc.textFile(\"/mnt/c/Users/Tarun/Desktop/DIC/HW-3 BONUS//pg71927.txt\")\n",
    "\n",
    "\n",
    "combinedfil = readf1.union(readf2)\n",
    "\n",
    "stop_words = [\"on\", \"or\", \"such\",\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren't\", \n",
    "              \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \n",
    "              \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\", \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \n",
    "              \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\", \"has\", \"hasn't\", \n",
    "              \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \n",
    "              \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \n",
    "              \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \n",
    "              \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \n",
    "              \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \n",
    "              \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \n",
    "              \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\",\n",
    "              \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\",\n",
    "              \"why\", \"why's\", \"with\", \"won't\", \"would\", \"wouldn't\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\",\n",
    "              \"yours\", \"yourself\", \"yourselves\"] \n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "word_counts = combinedfil.flatMap(lambda line: remove_punctuation(line).lower().split()) \\\n",
    "                         .map(lambda word: (word.strip(), 1)) \\\n",
    "                         .filter(lambda x: x[0] not in stop_words) \\\n",
    "                         .reduceByKey(lambda a, b: a + b) \\\n",
    "                         .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "word_counts.saveAsTextFile(\"extended_wcount.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20df33a6-530a-4362-a90d-e9c7bbb24abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word - said           -        Count - 739\n",
      "Word - one           -        Count - 684\n",
      "Word - iris           -        Count - 398\n",
      "Word - like           -        Count - 360\n",
      "Word - see           -        Count - 319\n",
      "Word - will           -        Count - 280\n",
      "Word - just           -        Count - 253\n",
      "Word - man           -        Count - 253\n",
      "Word - now           -        Count - 250\n",
      "Word - eyes           -        Count - 236\n",
      "Word - must           -        Count - 232\n",
      "Word - napier           -        Count - 232\n",
      "Word - venice           -        Count - 230\n",
      "Word - us           -        Count - 228\n",
      "Word - know           -        Count - 218\n",
      "Word - may           -        Count - 217\n",
      "Word - say           -        Count - 207\n",
      "Word - can           -        Count - 207\n",
      "Word - hilary           -        Count - 206\n",
      "Word - never           -        Count - 191\n",
      "Word - guy           -        Count - 191\n",
      "Word - two           -        Count - 185\n",
      "Word - first           -        Count - 183\n",
      "Word - even           -        Count - 176\n",
      "Word - gerald           -        Count - 176\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "with open('./extended_wcount.txt/part-00000', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "top_25 = lines[:25]\n",
    "\n",
    "for line in top_25:\n",
    "    word, count = line.strip(\"()\\n'\").split(', ')\n",
    "    print(\"Word - {}           -        Count - {}\".format(word.strip('\\''), count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c6bac-b759-4abe-988e-a729f07e71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text files as RDDs\n",
    "file1_rdd = sc.textFile(\"/mnt/c/Users/Tarun/Desktop/DIC/HW-3 BONUS/pg71885.txt\")\n",
    "file2_rdd = sc.textFile(\"/mnt/c/Users/Tarun/Desktop/DIC/HW-3 BONUS//pg71897.txt\")\n",
    "\n",
    "# Combine RDDs if needed\n",
    "combined_rdd = file1_rdd.union(file2_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95696f82-63f2-4a52-bd52-0d375a881feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform word count\n",
    "word_counts = combined_rdd.flatMap(lambda line: line.lower().split()) \\\n",
    "    .map(lambda word: (word.strip('.,;:\"!?()[]'), 1)) \\\n",
    "    .filter(lambda x: x[0] != '') \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Sort the word counts by count in descending order\n",
    "sorted_word_counts = word_counts.map(lambda x: (x[1], x[0])).sortByKey(False)\n",
    "\n",
    "# Collect the result\n",
    "result = sorted_word_counts.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686faf6-7a39-42f1-a1a1-c29cee1df44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to a text file\n",
    "output_file = \"/word_count_output.txt\"\n",
    "sorted_word_counts.saveAsTextFile(output_file)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67991aa-79ac-48b6-944b-20cfbb07f250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2615419a-1001-4017-b3c4-4facd6480411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1934eb-9cef-45f5-8fbf-c42bb60bee32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
